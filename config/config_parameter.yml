hyperparams_input:
  img_width_imagenet: 224
  img_height_imagenet: 224
  img_width_cifar10: 32
  img_height_cifar10: 32
  img_width_mnist: 28
  img_height_mnist: 28
  num_channel_rgb: 3
  num_channel_bw: 1
  num_class_imagenet: 1000
  num_class_cifar10: 10
  num_class_mnist: 10

hyperparams_single_train:
  num_epoch: 1
  random_seed: 10000
  model_type: resnet
  activation: relu
  optimizer: Adam
  batch_size: 32
  num_model_layer: 50
  learning_rate: 0.01
  train_dataset: cifar10
  use_cpu: False
  use_tb_timeline: False

hyperparams_multiple_train:
  num_epoch: 1
  random_seed: 10000
  model_type:
    - mobilenet
    - mobilenet
  # activation: sigmoid, leaky_relu, tanh, relu
  activation:
    - relu
    - relu
  # optimizers: Adam, SGD, Adagrad, Momentum
  optimizer:
    - Adam
    - SGD
  num_model_layer:
    - 1
    - 1
  # if batch size are not all same, make batch_padding True
  batch_size:
    - 32
    - 32
  learning_rate:
    - 0.00001
    - 0.00001
  train_dataset: cifar10
  batch_padding: False
  use_cpu: False
  same_input: True
  use_tb_timeline: False

hyperparams_hyperband:
  resource_conf: 4
  down_rate: 2
  pack_rate: 9
  train_dataset: cifar10
  # policy: none, pack-random, pack-bs, pack-knn
  schedule_policy: none
  # parameters for generating workload
  workload_random_seed: 10000
  model_type:
    - resnet-34
    - mobilenet-1
  activation:
    - relu
  optimizer:
    - Adam
  batch_size:
    - 20
    - 40
    - 60
  learning_rate:
    - 0.00001
    - 0.0001
    - 0.001
  model_type_a:
    - densenet-121
    - densenet-169
    - densenet-201
    - densenet-264
    - resnet-18
    - resnet-34
    - resnet-50
    - resnet-101
    - resnet-152
    - mobilenet-1
    - mlp-1
  activation_a:
    - sigmoid
    - leaky_relu
    - tanh
    - relu
  optimizer_a:
    - Adam
    - SGD
    - Adagrad
    - Momentum
  batch_size_a:
    - 20
    - 25
    - 30
    - 35
    - 40
    - 45
    - 50
    - 55
    - 60
    - 65
    - 70
  learning_rate_a:
    - 0.000001
    - 0.00001
    - 0.0001
    - 0.001
    - 0.01
    - 0.1
